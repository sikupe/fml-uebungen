{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "    def find_leaf(self, x) -> Node:\n",
    "        node = self.root\n",
    "        while hasattr(node, \"feature\"):\n",
    "            j = node.feature\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DensityTree, self).__init__()\n",
    "\n",
    "    def train(self, data, prior, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for the digit under consideration\n",
    "        prior: the prior probability of this digit\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        self.prior = prior\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D))  # number of features to consider for each split decision\n",
    "\n",
    "        # find and remember the tree's bounding box, \n",
    "        # i.e. the lower and upper limits of the training feature set\n",
    "        m, M = calc_bbox(data)\n",
    "        self.box = m, M\n",
    "\n",
    "        # identify invalid features and adjust the bounding box\n",
    "        # (If m[j] == M[j] for some j, the bounding box has zero volume, \n",
    "        #  causing divide-by-zero errors later on. We must exclude these\n",
    "        #  features from splitting and adjust the bounding box limits \n",
    "        #  such that invalid features have no effect on the volume.)\n",
    "        valid_features = np.where(m != M)[0]\n",
    "        invalid_features = np.where(m == M)[0]\n",
    "        M[invalid_features] = m[invalid_features] + 1\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.box = m.copy(), M.copy()\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0]  # number of instances in present node\n",
    "            if n >= n_min:\n",
    "                # Call 'make_density_split_node()' with 'D_try' randomly selected \n",
    "                # indices from 'valid_features'. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                indices = np.random.choice(np.arange(0, len(valid_features)), D_try)\n",
    "                left, right = make_density_split_node(node, N, valid_features[indices])\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                # Call 'make_density_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_density_leaf_node(node, N)\n",
    "\n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # return p(x | y) * p(y) if x is within the tree's bounding box \n",
    "        # and return 0 otherwise\n",
    "        return leaf.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_loo_error(N_m: int, N: int, V_m: float) -> float:\n",
    "    # print()\n",
    "    # print(f'N_m = {N_m}')\n",
    "    # print(f'N = {N}')\n",
    "    # print(f'V_m = {V_m}')\n",
    "    # print(f'-(2 * N_m * (N_m - 1)) = {-(2 * N_m * (N_m - 1))}')\n",
    "    # print(f'(N * (N - 1) * V_m) = {(N * (N - 1) * V_m)}')\n",
    "    # print(f'(N_m ** 2) = {(N_m ** 2)}')\n",
    "    # print(f'((N ** 2) * V_m) = {((N ** 2) * V_m)}')\n",
    "    # print(f'-(2 * N_m * (N_m - 1)) / (N * (N - 1) * V_m) = {-(2 * N_m * (N_m - 1)) / (N * (N - 1) * V_m)}')\n",
    "    # print(f'(N_m ** 2) / ((N ** 2) * V_m) = {(N_m ** 2) / ((N ** 2) * V_m)}')\n",
    "    # print()\n",
    "    return -(2 * N_m * (N_m - 1)) / (N * (N - 1) * V_m) + (N_m ** 2) / ((N ** 2) * V_m)\n",
    "\n",
    "\n",
    "def calc_volume(bounding_box: Tuple[np.ndarray, np.ndarray]):\n",
    "    m, M = bounding_box\n",
    "    return np.prod(M - m)\n",
    "\n",
    "\n",
    "def make_density_split_node(node, N, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    N:    the total number of training instances for the current class\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "    m, M = node.box\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = None, None\n",
    "\n",
    "    for j in feature_indices:\n",
    "        # Hint: For each feature considered, first remove duplicate feature values using \n",
    "        # 'np.unique()'. Describe here why this is necessary.\n",
    "\n",
    "        # It's necessary because otherwise if two instances have the same feature value the mean between them is the feature value self so the threshold would not be in the mid between feature values anymore\n",
    "\n",
    "        # np.unique returns an already sorted array\n",
    "        data_unique = np.unique(node.data[:, j])\n",
    "        # Compute candidate thresholds\n",
    "        tj = 0.5 * (data_unique[1:] + data_unique[:-1])\n",
    "\n",
    "        # Illustration: for loop - hint: vectorized version is possible\n",
    "        for t in tj:\n",
    "            # Compute the error\n",
    "            N_l = np.sum(node.data[:, j] <= t)\n",
    "            N_r = n - N_l\n",
    "\n",
    "            volume = calc_volume((m, M))\n",
    "            l_volume = volume * t / (M[j] - m[j])\n",
    "            r_volume = volume - l_volume\n",
    "\n",
    "            if l_volume != 0 and r_volume != 0:\n",
    "                loo_err_l = calc_loo_error(N_l, N, l_volume)\n",
    "                loo_err_r = calc_loo_error(N_r, N, r_volume)\n",
    "\n",
    "                loo_error = loo_err_l + loo_err_r\n",
    "\n",
    "                # print(f'loo_error: {loo_error}')\n",
    "\n",
    "                # choose the best threshold that\n",
    "                if loo_error < e_min:\n",
    "                    # print(f'e_min: {e_min}, j_min: {j_min}, t_min: {t_min}')\n",
    "                    e_min = loo_error\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "\n",
    "    # initialize 'left' and 'right' with the data subsets and bounding boxes\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[np.where(node.data[:, j_min] <= t_min), :]  # store data in left node -- for subsequent splits\n",
    "    left.box = m.copy(), M.copy()  # store bounding box in left node\n",
    "    left.box[1][j_min] = t_min\n",
    "    right.data = node.data[np.where(node.data[:, j_min] > t_min), :]\n",
    "    right.box = m.copy(), M.copy()\n",
    "    right.box[0][j_min] = t_min\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_leaf_node(node, N):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    N:    the total number of training instances for the current class\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    n = node.data.shape[0]\n",
    "    v = calc_volume(node.box)\n",
    "    node.response = n / (N * v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DecisionTree, self).__init__()\n",
    "\n",
    "    def train(self, data, labels, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for all digits\n",
    "        labels: the corresponding ground-truth responses\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D))  # how many features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0]  # number of instances in present node\n",
    "            if n >= n_min and not node_is_pure(node):\n",
    "                # Call 'make_decision_split_node()' with 'D_try' randomly selected \n",
    "                # feature indices. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                indices = np.random.choice(np.arange(0, D), D_try)\n",
    "                left, right = make_decision_split_node(node, indices)\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                # Call 'make_decision_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_decision_leaf_node(node)\n",
    "\n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # compute p(y | x)\n",
    "        return np.argmax(leaf.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_gini(N_l_k: np.ndarray, N_l: int) -> float:\n",
    "    return N_l * (1 - (1 / N_l) * np.sum(N_l_k ** 2))\n",
    "\n",
    "\n",
    "def calc_N_l_k(node_target: np.ndarray, tresholded_indices: np.ndarray) -> np.ndarray:\n",
    "    return np.array([np.sum(node_target[tresholded_indices] == cls) for cls in range(10)])\n",
    "\n",
    "\n",
    "def make_decision_split_node(node, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "\n",
    "    e_min = float('inf')\n",
    "    t_min, j_min = 0, 0\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    for j in feature_indices:\n",
    "        data_unique = np.unique(node.data[:, j])\n",
    "        tj = 0.5 * (data_unique[1:] + data_unique[:-1])\n",
    "\n",
    "        for t in tj:\n",
    "            indices_l = np.where(node.data[:, j] <= t)\n",
    "            indices_r = np.where(node.data[:, j] > t)\n",
    "\n",
    "            N_l_k_l = calc_N_l_k(node.labels, indices_l)\n",
    "            N_l_k_r = calc_N_l_k(node.labels, indices_r)\n",
    "\n",
    "            N_l_l = len(indices_l)\n",
    "            N_l_r = len(indices_r)\n",
    "\n",
    "            gini_l = calc_gini(N_l_k_l, N_l_l)\n",
    "            gini_r = calc_gini(N_l_k_r, N_l_r)\n",
    "\n",
    "            gini = gini_l + gini_r\n",
    "\n",
    "            if gini < e_min:\n",
    "                e_min = gini\n",
    "                t_min = t\n",
    "                j_min = j\n",
    "\n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "\n",
    "    # initialize 'left' and 'right' with the data subsets and labels\n",
    "    # according to the optimal split found above\n",
    "    indices_l = np.where(node.data[:, j_min] <= t_min)\n",
    "    indices_r = np.where(node.data[:, j_min] > t_min)\n",
    "\n",
    "    left.data = node.data[indices_l]  # data in left node\n",
    "    left.labels = node.labels[indices_l]  # corresponding labels\n",
    "    right.data = node.data[indices_r]\n",
    "    right.labels = node.labels[indices_r]\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_decision_leaf_node(node):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    node.N = len(node.labels)\n",
    "    node.response = np.bincount(node.labels, minlength=10) / node.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def node_is_pure(node):\n",
    "    '''\n",
    "    check if 'node' ontains only instances of the same digit\n",
    "    '''\n",
    "    return len(np.unique(node.labels)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 33 34 35 36 37 38 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 59 60 61 62 63]\n",
      "(1797, 61)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "def calc_bbox(data) -> (np.ndarray, np.ndarray):\n",
    "    return np.min(data, axis=0).copy(), np.max(data, axis=0).copy()\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]\n",
    "\n",
    "# Removing features where min value == max value == 0, because this feature does not contain any information (it's the same for all instances)\n",
    "smallest, biggest = calc_bbox(data)\n",
    "distances = biggest - smallest\n",
    "print(distances.shape)\n",
    "\n",
    "dims_with_information = np.where(distances > 0)[0]\n",
    "print(dims_with_information)\n",
    "\n",
    "data = data[:, dims_with_information]\n",
    "print(data.shape)\n",
    "\n",
    "# Normalizing to values between 0 and 2\n",
    "# data = data / 16\n",
    "# print(calc_bbox(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density tree:\n",
      "n_min: 40\n",
      "0.9037284362826934\n",
      "n_min: 30\n",
      "0.8330550918196995\n",
      "n_min: 20\n",
      "0.899276572064552\n",
      "n_min: 10\n",
      "0.9321090706733445\n",
      "n_min: 5\n",
      "0.9204229271007234\n",
      "\n",
      "Decision tree:\n",
      "n_min: 40\n",
      "0.2648859209794101\n",
      "n_min: 30\n",
      "0.32276015581524764\n",
      "n_min: 20\n",
      "0.25598219254312743\n",
      "n_min: 10\n",
      "0.20534223706176963\n",
      "n_min: 5\n",
      "0.09515859766277128\n"
     ]
    }
   ],
   "source": [
    "# train trees, plot training error confusion matrices, and comment on your results\n",
    "def train_test_density_tree(n_min):\n",
    "    density_trees: List[DensityTree] = []\n",
    "    for number in range(10):\n",
    "        indices = np.where(target == number)\n",
    "        filtered_data = data[indices]\n",
    "        prior = len(filtered_data) / len(data)\n",
    "\n",
    "        density_tree = DensityTree()\n",
    "        density_tree.train(filtered_data, prior, n_min)\n",
    "        density_trees.append(density_tree)\n",
    "\n",
    "    calculated_target = np.zeros(len(target))\n",
    "\n",
    "    for i, instance in enumerate(data):\n",
    "        p_max = -1\n",
    "        num_max = -1\n",
    "        for number, tree in enumerate(density_trees):\n",
    "            p = tree.predict(instance)\n",
    "            if p > p_max:\n",
    "                p_max = p\n",
    "                num_max = number\n",
    "        calculated_target[i] = num_max\n",
    "\n",
    "    # print(target)\n",
    "    # print(calculated_target)\n",
    "    density_tree_err = calculated_target != target\n",
    "    # print(density_tree_err)\n",
    "\n",
    "    density_tree_err_rate = np.sum(density_tree_err) / len(target)\n",
    "    print(density_tree_err_rate)\n",
    "\n",
    "\n",
    "def train_test_decision_tree(n_min):\n",
    "    decision_tree = DecisionTree()\n",
    "    decision_tree.train(data, target, n_min)\n",
    "\n",
    "    calculated_target = np.array([decision_tree.predict(instance) for instance in data])\n",
    "\n",
    "    decision_tree_err = calculated_target != target\n",
    "    # print(decision_tree_err)\n",
    "\n",
    "    decision_tree_err_rate = np.sum(decision_tree_err) / len(target)\n",
    "    print(decision_tree_err_rate)\n",
    "\n",
    "\n",
    "print('Density tree:')\n",
    "for hyper_n_min in [40, 30, 20, 10, 5]:\n",
    "    print(f'n_min: {hyper_n_min}')\n",
    "    train_test_density_tree(hyper_n_min)\n",
    "\n",
    "\n",
    "print()\n",
    "print('Decision tree:')\n",
    "for hyper_n_min in [40, 30, 20, 10, 5]:\n",
    "    print(f'n_min: {hyper_n_min}')\n",
    "    train_test_decision_tree(hyper_n_min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DensityForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DensityTree() for i in range(n_trees)]\n",
    "\n",
    "    def train(self, data, prior, n_min=20):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            ...  # your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return ...  # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DecisionTree() for i in range(n_trees)]\n",
    "\n",
    "    def train(self, data, labels, n_min=0):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            ...  # your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return ...  # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
    "...  # your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}