{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "    def find_leaf(self, x) -> Node:\n",
    "        node = self.root\n",
    "        while hasattr(node, \"feature\"):\n",
    "            j = node.feature\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DensityTree, self).__init__()\n",
    "\n",
    "    def train(self, data, prior, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for the digit under consideration\n",
    "        prior: the prior probability of this digit\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        self.prior = prior\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D))  # number of features to consider for each split decision\n",
    "\n",
    "        # find and remember the tree's bounding box, \n",
    "        # i.e. the lower and upper limits of the training feature set\n",
    "        m, M = calc_bbox(data)\n",
    "        self.box = m, M\n",
    "\n",
    "        # identify invalid features and adjust the bounding box\n",
    "        # (If m[j] == M[j] for some j, the bounding box has zero volume, \n",
    "        #  causing divide-by-zero errors later on. We must exclude these\n",
    "        #  features from splitting and adjust the bounding box limits \n",
    "        #  such that invalid features have no effect on the volume.)\n",
    "        valid_features = np.where(m != M)[0]\n",
    "        invalid_features = np.where(m == M)[0]\n",
    "        M[invalid_features] = m[invalid_features] + 1\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.box = m.copy(), M.copy()\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0]  # number of instances in present node\n",
    "            if n >= n_min:\n",
    "                # Call 'make_density_split_node()' with 'D_try' randomly selected \n",
    "                # indices from 'valid_features'. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                indices = np.random.choice(np.arange(0, len(valid_features)), D_try, False)\n",
    "                left, right = make_density_split_node(node, N, valid_features[indices])\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                # Call 'make_density_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_density_leaf_node(node, N)\n",
    "\n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # return p(x | y) * p(y) if x is within the tree's bounding box \n",
    "        # and return 0 otherwise\n",
    "\n",
    "        if np.sum(x < self.box[0]) > 0 or np.sum(x > self.box[1]) > 0:\n",
    "            return 0.0\n",
    "        return self.prior * leaf.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_loo_error(N_m: int, N: int, V_m: float) -> float:\n",
    "    # print()\n",
    "    # print(f'N_m = {N_m}')\n",
    "    # print(f'N = {N}')\n",
    "    # print(f'V_m = {V_m}')\n",
    "    # print(f'-(2 * N_m * (N_m - 1)) = {-(2 * N_m * (N_m - 1))}')\n",
    "    # print(f'(N * (N - 1) * V_m) = {(N * (N - 1) * V_m)}')\n",
    "    # print(f'(N_m ** 2) = {(N_m ** 2)}')\n",
    "    # print(f'((N ** 2) * V_m) = {((N ** 2) * V_m)}')\n",
    "    # print(f'-(2 * N_m * (N_m - 1)) / (N * (N - 1) * V_m) = {-(2 * N_m * (N_m - 1)) / (N * (N - 1) * V_m)}')\n",
    "    # print(f'(N_m ** 2) / ((N ** 2) * V_m) = {(N_m ** 2) / ((N ** 2) * V_m)}')\n",
    "    # print()\n",
    "    return -(2 * N_m * (N_m - 1)) / (N * (N - 1) * V_m) + (N_m ** 2) / ((N ** 2) * V_m)\n",
    "\n",
    "\n",
    "def calc_volume(bounding_box: Tuple[np.ndarray, np.ndarray]):\n",
    "    m, M = bounding_box\n",
    "    return np.prod(M - m)\n",
    "\n",
    "\n",
    "def make_density_split_node(node, N, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    N:    the total number of training instances for the current class\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "    m, M = node.box\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = 0, 0\n",
    "\n",
    "    volume = calc_volume((m, M))\n",
    "\n",
    "    for j in feature_indices:\n",
    "        # Hint: For each feature considered, first remove duplicate feature values using\n",
    "        # 'np.unique()'. Describe here why this is necessary.\n",
    "\n",
    "        # It's necessary because otherwise if two instances have the same feature value the mean between them is the feature value self so the threshold would not be in the mid between feature values anymore\n",
    "\n",
    "        # np.unique returns an already sorted array\n",
    "        data_unique = np.unique(node.data[:, j])\n",
    "        # Compute candidate thresholds\n",
    "        tj = 0.5 * (data_unique[1:] + data_unique[:-1])\n",
    "\n",
    "        # Illustration: for loop - hint: vectorized version is possible\n",
    "        for t in tj:\n",
    "            # Compute the error\n",
    "            N_l = np.sum(node.data[:, j] <= t)\n",
    "            N_r = n - N_l\n",
    "\n",
    "            l_volume = volume * t / (M[j] - m[j])\n",
    "            r_volume = volume - l_volume\n",
    "\n",
    "            if l_volume != 0 and r_volume != 0:\n",
    "                loo_err_l = calc_loo_error(N_l, N, l_volume)\n",
    "                loo_err_r = calc_loo_error(N_r, N, r_volume)\n",
    "\n",
    "                loo_error = loo_err_l + loo_err_r\n",
    "\n",
    "                # print(f'loo_error: {loo_error}')\n",
    "\n",
    "                # choose the best threshold that\n",
    "                if loo_error < e_min:\n",
    "                    # print(f'e_min: {e_min}, j_min: {j_min}, t_min: {t_min}')\n",
    "                    e_min = loo_error\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "\n",
    "    # initialize 'left' and 'right' with the data subsets and bounding boxes\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[node.data[:, j_min] <= t_min, :]  # store data in left node -- for subsequent splits\n",
    "    left.box = m.copy(), M.copy()  # store bounding box in left node\n",
    "    left.box[1][j_min] = t_min\n",
    "    right.data = node.data[node.data[:, j_min] > t_min, :]\n",
    "    right.box = m.copy(), M.copy()\n",
    "    right.box[0][j_min] = t_min\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_leaf_node(node, N):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    N:    the total number of training instances for the current class\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    n = node.data.shape[0]\n",
    "    v = calc_volume(node.box)\n",
    "    node.response = n / (N * v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DecisionTree, self).__init__()\n",
    "\n",
    "    def train(self, data, labels, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for all digits\n",
    "        labels: the corresponding ground-truth responses\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D))  # how many features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0]  # number of instances in present node\n",
    "            if n >= n_min and not node_is_pure(node):\n",
    "                # Call 'make_decision_split_node()' with 'D_try' randomly selected \n",
    "                # feature indices. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                indices = np.random.choice(np.arange(0, D), D_try, False)\n",
    "                left, right = make_decision_split_node(node, indices)\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                # Call 'make_decision_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_decision_leaf_node(node)\n",
    "\n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # compute p(y | x)\n",
    "        return np.argmax(leaf.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_gini(N_l_k: np.ndarray, N_l: int) -> float:\n",
    "    return N_l * (1 - np.sum((N_l_k / N_l) ** 2))\n",
    "\n",
    "\n",
    "def calc_N_l_k(node_target: np.ndarray, tresholded_indices: np.ndarray) -> np.ndarray:\n",
    "    return np.bincount(node_target[tresholded_indices], minlength=10)\n",
    "\n",
    "\n",
    "def make_decision_split_node(node, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "\n",
    "    e_min = float('inf')\n",
    "    t_min, j_min = 0, 0\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    for j in feature_indices:\n",
    "        data_unique = np.unique(node.data[:, j])\n",
    "        tj = 0.5 * (data_unique[1:] + data_unique[:-1])\n",
    "        \n",
    "        if len(data_unique) > 1:\n",
    "            for t in tj:\n",
    "                indices_l = np.where(node.data[:, j] <= t)\n",
    "                indices_r = np.where(node.data[:, j] > t)\n",
    "\n",
    "                N_l_k_l = calc_N_l_k(node.labels, indices_l)\n",
    "                N_l_k_r = calc_N_l_k(node.labels, indices_r)\n",
    "\n",
    "                N_l_l = len(indices_l)\n",
    "                N_l_r = len(indices_r)\n",
    "\n",
    "                gini_l = calc_gini(N_l_k_l, N_l_l)\n",
    "                gini_r = calc_gini(N_l_k_r, N_l_r)\n",
    "\n",
    "                gini = gini_l + gini_r\n",
    "\n",
    "                if gini < e_min:\n",
    "                    e_min = gini\n",
    "                    t_min = t\n",
    "                    j_min = j\n",
    "\n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "\n",
    "    # initialize 'left' and 'right' with the data subsets and labels\n",
    "    # according to the optimal split found above\n",
    "    indices_l = np.where(node.data[:, j_min] <= t_min)\n",
    "    indices_r = np.where(node.data[:, j_min] > t_min)\n",
    "\n",
    "    left.data = node.data[indices_l]  # data in left node\n",
    "    left.labels = node.labels[indices_l]  # corresponding labels\n",
    "    right.data = node.data[indices_r]\n",
    "    right.labels = node.labels[indices_r]\n",
    "\n",
    "    if len(left.data) == 0 or len(right.data) == 0:\n",
    "        print('Data empty')\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_decision_leaf_node(node):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    node.N = len(node.labels)\n",
    "    node.response = np.bincount(node.labels, minlength=10) / node.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def node_is_pure(node):\n",
    "    '''\n",
    "    check if 'node' ontains only instances of the same digit\n",
    "    '''\n",
    "    return len(np.unique(node.labels)) <= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 33 34 35 36 37 38 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 59 60 61 62 63]\n",
      "(1797, 61)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "def calc_bbox(data) -> (np.ndarray, np.ndarray):\n",
    "    return np.min(data, axis=0).copy(), np.max(data, axis=0).copy()\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]\n",
    "\n",
    "# Removing features where min value == max value == 0, because this feature does not contain any information (it's the same for all instances)\n",
    "smallest, biggest = calc_bbox(data)\n",
    "distances = biggest - smallest\n",
    "print(distances.shape)\n",
    "\n",
    "dims_with_information = np.where(distances > 0)[0]\n",
    "print(dims_with_information)\n",
    "\n",
    "data = data[:, dims_with_information]\n",
    "print(data.shape)\n",
    "\n",
    "# Normalizing to values between 0 and 2\n",
    "# data = data / 16\n",
    "# print(calc_bbox(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative classifier:\n",
      "n_min: 20\n",
      "0.18586533110740122\n",
      "actual/expected    0         1         2         3         4         5    6  \\\n",
      "0                1.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "1                0.0  0.714286  0.028249  0.010929  0.016575  0.005495  0.0   \n",
      "2                0.0  0.104396  0.621469  0.005464  0.000000  0.000000  0.0   \n",
      "3                0.0  0.021978  0.225989  0.699454  0.000000  0.098901  0.0   \n",
      "4                0.0  0.027473  0.000000  0.000000  0.856354  0.000000  0.0   \n",
      "5                0.0  0.000000  0.016949  0.000000  0.011050  0.741758  0.0   \n",
      "6                0.0  0.000000  0.000000  0.000000  0.000000  0.000000  1.0   \n",
      "7                0.0  0.010989  0.000000  0.016393  0.104972  0.010989  0.0   \n",
      "8                0.0  0.098901  0.107345  0.213115  0.005525  0.060440  0.0   \n",
      "9                0.0  0.021978  0.000000  0.054645  0.005525  0.082418  0.0   \n",
      "\n",
      "actual/expected         7         8         9  \n",
      "0                0.000000  0.000000  0.000000  \n",
      "1                0.000000  0.051724  0.027778  \n",
      "2                0.000000  0.028736  0.000000  \n",
      "3                0.016760  0.068966  0.105556  \n",
      "4                0.016760  0.000000  0.027778  \n",
      "5                0.000000  0.000000  0.000000  \n",
      "6                0.000000  0.000000  0.000000  \n",
      "7                0.960894  0.005747  0.022222  \n",
      "8                0.005587  0.844828  0.111111  \n",
      "9                0.000000  0.000000  0.705556  \n",
      "n_min: 10\n",
      "0.1930996104618809\n",
      "actual/expected    0         1         2         3         4         5    6  \\\n",
      "0                1.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "1                0.0  0.758242  0.022599  0.016393  0.011050  0.000000  0.0   \n",
      "2                0.0  0.054945  0.683616  0.060109  0.000000  0.000000  0.0   \n",
      "3                0.0  0.000000  0.079096  0.590164  0.000000  0.032967  0.0   \n",
      "4                0.0  0.010989  0.000000  0.000000  0.773481  0.000000  0.0   \n",
      "5                0.0  0.021978  0.000000  0.054645  0.011050  0.912088  0.0   \n",
      "6                0.0  0.000000  0.000000  0.000000  0.000000  0.000000  1.0   \n",
      "7                0.0  0.016484  0.000000  0.021858  0.193370  0.016484  0.0   \n",
      "8                0.0  0.131868  0.209040  0.234973  0.000000  0.027473  0.0   \n",
      "9                0.0  0.005495  0.005650  0.021858  0.011050  0.010989  0.0   \n",
      "\n",
      "actual/expected         7         8         9  \n",
      "0                0.000000  0.000000  0.000000  \n",
      "1                0.000000  0.091954  0.033333  \n",
      "2                0.000000  0.005747  0.011111  \n",
      "3                0.000000  0.000000  0.138889  \n",
      "4                0.005587  0.000000  0.038889  \n",
      "5                0.005587  0.034483  0.022222  \n",
      "6                0.000000  0.000000  0.000000  \n",
      "7                0.983240  0.068966  0.072222  \n",
      "8                0.005587  0.798851  0.111111  \n",
      "9                0.000000  0.000000  0.572222  \n",
      "n_min: 5\n",
      "0.17195325542570952\n",
      "actual/expected    0         1         2         3         4         5    6  \\\n",
      "0                1.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "1                0.0  0.741758  0.016949  0.010929  0.016575  0.021978  0.0   \n",
      "2                0.0  0.032967  0.751412  0.010929  0.000000  0.000000  0.0   \n",
      "3                0.0  0.005495  0.135593  0.639344  0.000000  0.021978  0.0   \n",
      "4                0.0  0.000000  0.000000  0.000000  0.812155  0.000000  0.0   \n",
      "5                0.0  0.016484  0.005650  0.043716  0.022099  0.824176  0.0   \n",
      "6                0.0  0.000000  0.000000  0.000000  0.000000  0.000000  1.0   \n",
      "7                0.0  0.049451  0.000000  0.016393  0.138122  0.016484  0.0   \n",
      "8                0.0  0.148352  0.090395  0.163934  0.005525  0.076923  0.0   \n",
      "9                0.0  0.005495  0.000000  0.114754  0.005525  0.038462  0.0   \n",
      "\n",
      "actual/expected         7         8         9  \n",
      "0                0.000000  0.000000  0.000000  \n",
      "1                0.011173  0.080460  0.038889  \n",
      "2                0.000000  0.005747  0.005556  \n",
      "3                0.000000  0.028736  0.111111  \n",
      "4                0.005587  0.000000  0.005556  \n",
      "5                0.016760  0.017241  0.016667  \n",
      "6                0.000000  0.000000  0.000000  \n",
      "7                0.955307  0.022989  0.033333  \n",
      "8                0.011173  0.844828  0.072222  \n",
      "9                0.000000  0.000000  0.716667  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discriminative classifier:\n",
      "n_min: 20\n",
      "0.2615470228158041\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.893258  0.005495  0.016949  0.005464  0.038674  0.000000   \n",
      "1                0.016854  0.824176  0.090395  0.010929  0.082873  0.049451   \n",
      "2                0.000000  0.010989  0.587571  0.071038  0.005525  0.027473   \n",
      "3                0.000000  0.038462  0.135593  0.715847  0.000000  0.054945   \n",
      "4                0.022472  0.005495  0.011299  0.005464  0.850829  0.043956   \n",
      "5                0.011236  0.021978  0.073446  0.060109  0.000000  0.763736   \n",
      "6                0.022472  0.005495  0.016949  0.038251  0.000000  0.000000   \n",
      "7                0.005618  0.000000  0.000000  0.000000  0.022099  0.010989   \n",
      "8                0.011236  0.054945  0.016949  0.043716  0.000000  0.032967   \n",
      "9                0.016854  0.032967  0.050847  0.049180  0.000000  0.016484   \n",
      "\n",
      "actual/expected         6         7         8         9  \n",
      "0                0.000000  0.016760  0.022989  0.050000  \n",
      "1                0.011050  0.011173  0.063218  0.050000  \n",
      "2                0.022099  0.039106  0.086207  0.050000  \n",
      "3                0.005525  0.011173  0.080460  0.144444  \n",
      "4                0.016575  0.094972  0.040230  0.022222  \n",
      "5                0.000000  0.055866  0.040230  0.077778  \n",
      "6                0.944751  0.000000  0.011494  0.055556  \n",
      "7                0.000000  0.726257  0.011494  0.016667  \n",
      "8                0.000000  0.016760  0.603448  0.066667  \n",
      "9                0.000000  0.027933  0.040230  0.466667  \n",
      "n_min: 10\n",
      "0.2426265998887034\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.932584  0.000000  0.000000  0.000000  0.033149  0.000000   \n",
      "1                0.000000  0.895604  0.045198  0.016393  0.033149  0.021978   \n",
      "2                0.000000  0.021978  0.802260  0.087432  0.038674  0.032967   \n",
      "3                0.011236  0.005495  0.050847  0.693989  0.005525  0.060440   \n",
      "4                0.000000  0.005495  0.005650  0.000000  0.762431  0.016484   \n",
      "5                0.000000  0.038462  0.028249  0.054645  0.027624  0.769231   \n",
      "6                0.050562  0.000000  0.016949  0.049180  0.038674  0.027473   \n",
      "7                0.000000  0.005495  0.000000  0.010929  0.033149  0.021978   \n",
      "8                0.005618  0.010989  0.011299  0.021858  0.005525  0.010989   \n",
      "9                0.000000  0.016484  0.039548  0.065574  0.022099  0.038462   \n",
      "\n",
      "actual/expected         6         7         8         9  \n",
      "0                0.005525  0.016760  0.005747  0.011111  \n",
      "1                0.038674  0.050279  0.057471  0.027778  \n",
      "2                0.027624  0.044693  0.080460  0.066667  \n",
      "3                0.011050  0.033520  0.068966  0.094444  \n",
      "4                0.005525  0.044693  0.017241  0.011111  \n",
      "5                0.011050  0.072626  0.068966  0.072222  \n",
      "6                0.878453  0.016760  0.011494  0.050000  \n",
      "7                0.000000  0.659218  0.005747  0.022222  \n",
      "8                0.005525  0.022346  0.609195  0.077778  \n",
      "9                0.016575  0.039106  0.074713  0.566667  \n",
      "n_min: 5\n",
      "0.10239287701725097\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.960674  0.005495  0.005650  0.016393  0.027624  0.010989   \n",
      "1                0.005618  0.978022  0.028249  0.010929  0.011050  0.016484   \n",
      "2                0.005618  0.000000  0.937853  0.027322  0.005525  0.010989   \n",
      "3                0.000000  0.000000  0.011299  0.896175  0.000000  0.021978   \n",
      "4                0.005618  0.010989  0.005650  0.000000  0.950276  0.016484   \n",
      "5                0.000000  0.005495  0.000000  0.005464  0.000000  0.890110   \n",
      "6                0.005618  0.000000  0.005650  0.005464  0.000000  0.005495   \n",
      "7                0.005618  0.000000  0.000000  0.010929  0.005525  0.021978   \n",
      "8                0.005618  0.000000  0.005650  0.027322  0.000000  0.000000   \n",
      "9                0.005618  0.000000  0.000000  0.000000  0.000000  0.005495   \n",
      "\n",
      "actual/expected         6         7         8         9  \n",
      "0                0.005525  0.000000  0.011494  0.038889  \n",
      "1                0.016575  0.011173  0.028736  0.011111  \n",
      "2                0.000000  0.027933  0.034483  0.033333  \n",
      "3                0.011050  0.016760  0.028736  0.038889  \n",
      "4                0.016575  0.039106  0.000000  0.011111  \n",
      "5                0.000000  0.050279  0.022989  0.016667  \n",
      "6                0.950276  0.005587  0.005747  0.011111  \n",
      "7                0.000000  0.837989  0.034483  0.027778  \n",
      "8                0.000000  0.005587  0.821839  0.061111  \n",
      "9                0.000000  0.005587  0.011494  0.750000  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train trees, plot training error confusion matrices, and comment on your results\n",
    "\n",
    "def calc_confusion_matrix(calculated_target, expected) -> pd.DataFrame:\n",
    "    matrix = np.zeros((10, 10))\n",
    "    matrix_dict = {}\n",
    "    for expected_num in range(10):\n",
    "        expected_indices = np.where(expected == expected_num)[0]\n",
    "        calculated_values = calculated_target[expected_indices]\n",
    "        calc_bins = np.bincount(calculated_values, minlength=10)\n",
    "        matrix[expected_num] = calc_bins / len(expected_indices)\n",
    "        matrix_dict[expected_num] = calc_bins / len(expected_indices)\n",
    "\n",
    "    data_frame = pd.DataFrame(matrix_dict)\n",
    "    data_frame.columns.name = 'actual/expected'\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def show_plot(errors: np.ndarray, n_mins: np.ndarray):\n",
    "    x = range(len(n_mins))\n",
    "    x_ticks = np.arange(len(n_mins))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x, errors)\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(n_mins)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_test_generative_classifier(density_trees, n_min) -> float:\n",
    "    for number, density_tree in enumerate(density_trees):\n",
    "        indices = np.where(target == number)\n",
    "        filtered_data = data[indices]\n",
    "        prior = len(filtered_data) / len(data)\n",
    "\n",
    "        density_tree.train(filtered_data, prior, n_min)\n",
    "\n",
    "    calculated_target = np.zeros(len(target), dtype=int)\n",
    "\n",
    "    for i, instance in enumerate(data):\n",
    "        p_max = -1\n",
    "        num_max = -1\n",
    "        for number, tree in enumerate(density_trees):\n",
    "            p = tree.predict(instance)\n",
    "            if p > p_max:\n",
    "                p_max = p\n",
    "                num_max = number\n",
    "        calculated_target[i] = num_max\n",
    "\n",
    "    # print(target)\n",
    "    # print(calculated_target)\n",
    "    density_tree_err = calculated_target != target\n",
    "    # print(density_tree_err)\n",
    "\n",
    "    density_tree_err_rate = np.sum(density_tree_err) / len(target)\n",
    "    confusion_matrix = calc_confusion_matrix(calculated_target, target)\n",
    "    print(density_tree_err_rate)\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    return density_tree_err_rate\n",
    "\n",
    "\n",
    "def train_test_discriminative_classifier(decision_tree, n_min) -> float:\n",
    "    decision_tree.train(data, target, n_min)\n",
    "\n",
    "    calculated_target = np.array([decision_tree.predict(instance) for instance in data], dtype=int)\n",
    "\n",
    "    decision_tree_err = calculated_target != target\n",
    "    # print(decision_tree_err)\n",
    "\n",
    "    decision_tree_err_rate = np.sum(decision_tree_err) / len(target)\n",
    "    confusion_matrix = calc_confusion_matrix(calculated_target, target)\n",
    "    print(decision_tree_err_rate)\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    return decision_tree_err_rate\n",
    "\n",
    "\n",
    "def train_test(create_generative_classifiers, create_discriminative_classifier, generative_n_mins,\n",
    "               discriminative_n_mins):\n",
    "    density_tree_error = []\n",
    "    print('Generative classifier:')\n",
    "    for hyper_n_min in generative_n_mins:\n",
    "        print(f'n_min: {hyper_n_min}')\n",
    "        density_trees = create_generative_classifiers()\n",
    "        error = train_test_generative_classifier(density_trees, hyper_n_min)\n",
    "        density_tree_error.append(error)\n",
    "    show_plot(density_tree_error, generative_n_mins)\n",
    "\n",
    "    decision_tree_error = []\n",
    "    print()\n",
    "    print('Discriminative classifier:')\n",
    "    for hyper_n_min in discriminative_n_mins:\n",
    "        print(f'n_min: {hyper_n_min}')\n",
    "        error = train_test_discriminative_classifier(create_discriminative_classifier(), hyper_n_min)\n",
    "        decision_tree_error.append(error)\n",
    "    show_plot(decision_tree_error, discriminative_n_mins)\n",
    "\n",
    "\n",
    "train_test(lambda: [DensityTree() for _ in range(10)], lambda: DecisionTree(), [20, 10, 5], [20, 10, 5])\n",
    "\n",
    "#train_test(lambda: [], lambda: DecisionTree(), [], [0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The adjustment of the hyperparameter has the same form for both the density tree and the decision tree. Thus the deviation in the errors is for the decision tree broader.\n",
    "Both perform best with smaller n_min and the decision tree with n_min = 5 performs best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DensityForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DensityTree() for i in range(n_trees)]\n",
    "\n",
    "    def train(self, data, prior, n_min=20):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "\n",
    "            train_indices = np.random.choice(np.arange(len(data)), len(data), True)\n",
    "            train_data = data[train_indices]\n",
    "            tree.train(train_data, prior, n_min)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return np.sum(np.array([tree.predict(x) for tree in self.trees])) / len(self.trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DecisionTree() for i in range(n_trees)]\n",
    "\n",
    "    def train(self, data, labels, n_min=0):\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            # train each tree, using a bootstrap sample of the da\n",
    "            print(f'Training decision tree {i}')\n",
    "            train_indices = np.random.choice(range(len(data)), len(data), True)\n",
    "            train_data = data[train_indices]\n",
    "            train_labels = labels[train_indices]\n",
    "            tree.train(train_data, train_labels, n_min)\n",
    "            print(f'Decision tree {i} trained')\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return np.argmax(np.bincount([tree.predict(x) for tree in self.trees], minlength=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative classifier:\n",
      "n_min: 20\n",
      "0.10461880912632164\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.994382  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1                0.000000  0.890110  0.028249  0.005464  0.005525  0.000000   \n",
      "2                0.000000  0.038462  0.819209  0.000000  0.000000  0.000000   \n",
      "3                0.000000  0.000000  0.022599  0.885246  0.000000  0.076923   \n",
      "4                0.005618  0.016484  0.000000  0.000000  0.966851  0.005495   \n",
      "5                0.000000  0.010989  0.000000  0.005464  0.000000  0.851648   \n",
      "6                0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7                0.000000  0.010989  0.000000  0.016393  0.027624  0.005495   \n",
      "8                0.000000  0.016484  0.129944  0.081967  0.000000  0.043956   \n",
      "9                0.000000  0.016484  0.000000  0.005464  0.000000  0.016484   \n",
      "\n",
      "actual/expected    6         7         8         9  \n",
      "0                0.0  0.000000  0.000000  0.000000  \n",
      "1                0.0  0.005587  0.080460  0.011111  \n",
      "2                0.0  0.000000  0.000000  0.000000  \n",
      "3                0.0  0.000000  0.000000  0.155556  \n",
      "4                0.0  0.005587  0.000000  0.000000  \n",
      "5                0.0  0.005587  0.005747  0.005556  \n",
      "6                1.0  0.000000  0.000000  0.000000  \n",
      "7                0.0  0.983240  0.063218  0.050000  \n",
      "8                0.0  0.000000  0.850575  0.066667  \n",
      "9                0.0  0.000000  0.000000  0.711111  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discriminative classifier:\n",
      "n_min: 0\n",
      "Training decision tree 0\n",
      "Decision tree 0 trained\n",
      "Training decision tree 1\n",
      "Decision tree 1 trained\n",
      "Training decision tree 2\n",
      "Decision tree 2 trained\n",
      "Training decision tree 3\n",
      "Decision tree 3 trained\n",
      "Training decision tree 4\n",
      "Decision tree 4 trained\n",
      "Training decision tree 5\n",
      "Decision tree 5 trained\n",
      "Training decision tree 6\n",
      "Decision tree 6 trained\n",
      "Training decision tree 7\n",
      "Decision tree 7 trained\n",
      "Training decision tree 8\n",
      "Decision tree 8 trained\n",
      "Training decision tree 9\n",
      "Decision tree 9 trained\n",
      "Training decision tree 10\n",
      "Decision tree 10 trained\n",
      "Training decision tree 11\n",
      "Data empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22766/981153019.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  node.response = np.bincount(node.labels, minlength=10) / node.N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree 11 trained\n",
      "Training decision tree 12\n",
      "Decision tree 12 trained\n",
      "Training decision tree 13\n",
      "Decision tree 13 trained\n",
      "Training decision tree 14\n",
      "Decision tree 14 trained\n",
      "Training decision tree 15\n",
      "Decision tree 15 trained\n",
      "Training decision tree 16\n",
      "Decision tree 16 trained\n",
      "Training decision tree 17\n",
      "Decision tree 17 trained\n",
      "Training decision tree 18\n",
      "Decision tree 18 trained\n",
      "Training decision tree 19\n",
      "Data empty\n",
      "Decision tree 19 trained\n",
      "0.0005564830272676684\n",
      "actual/expected    0    1    2    3    4    5    6    7         8    9\n",
      "0                1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0\n",
      "1                0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0\n",
      "2                0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0\n",
      "3                0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.005747  0.0\n",
      "4                0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.000000  0.0\n",
      "5                0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.000000  0.0\n",
      "6                0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.000000  0.0\n",
      "7                0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.000000  0.0\n",
      "8                0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.994253  0.0\n",
      "9                0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
    "train_test(lambda: [DensityForest(20) for _ in range(10)], lambda: DecisionForest(20), [20], [0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}