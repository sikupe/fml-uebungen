{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes\n",
    "\n",
    "class Node:\n",
    "    pass\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "    def find_leaf(self, x) -> Node:\n",
    "        node = self.root\n",
    "        while hasattr(node, \"feature\"):\n",
    "            j = node.feature\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DensityTree, self).__init__()\n",
    "\n",
    "    def train(self, data, prior, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for the digit under consideration\n",
    "        prior: the prior probability of this digit\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        self.prior = prior\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D))  # number of features to consider for each split decision\n",
    "\n",
    "        # find and remember the tree's bounding box, \n",
    "        # i.e. the lower and upper limits of the training feature set\n",
    "        m, M = calc_bbox(data)\n",
    "        self.box = m, M\n",
    "\n",
    "        # identify invalid features and adjust the bounding box\n",
    "        # (If m[j] == M[j] for some j, the bounding box has zero volume, \n",
    "        #  causing divide-by-zero errors later on. We must exclude these\n",
    "        #  features from splitting and adjust the bounding box limits \n",
    "        #  such that invalid features have no effect on the volume.)\n",
    "        valid_features = np.where(m != M)[0]\n",
    "        invalid_features = np.where(m == M)[0]\n",
    "        M[invalid_features] = m[invalid_features] + 1\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.box = m.copy(), M.copy()\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0]  # number of instances in present node\n",
    "            if n >= n_min:\n",
    "                # Call 'make_density_split_node()' with 'D_try' randomly selected \n",
    "                # indices from 'valid_features'. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                indices = np.random.choice(np.arange(0, len(valid_features)), D_try)\n",
    "                left, right = make_density_split_node(node, N, valid_features[indices])\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                # Call 'make_density_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_density_leaf_node(node, N)\n",
    "\n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # return p(x | y) * p(y) if x is within the tree's bounding box \n",
    "        # and return 0 otherwise\n",
    "\n",
    "        if np.sum(x < self.box[0]) > 0 or np.sum(x > self.box[1]) > 0:\n",
    "            return 0.0\n",
    "        return self.prior * leaf.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_loo_error(N_m: int, N: int, V_m: float) -> float:\n",
    "    # print()\n",
    "    # print(f'N_m = {N_m}')\n",
    "    # print(f'N = {N}')\n",
    "    # print(f'V_m = {V_m}')\n",
    "    # print(f'-(2 * N_m * (N_m - 1)) = {-(2 * N_m * (N_m - 1))}')\n",
    "    # print(f'(N * (N - 1) * V_m) = {(N * (N - 1) * V_m)}')\n",
    "    # print(f'(N_m ** 2) = {(N_m ** 2)}')\n",
    "    # print(f'((N ** 2) * V_m) = {((N ** 2) * V_m)}')\n",
    "    # print(f'-(2 * N_m * (N_m - 1)) / (N * (N - 1) * V_m) = {-(2 * N_m * (N_m - 1)) / (N * (N - 1) * V_m)}')\n",
    "    # print(f'(N_m ** 2) / ((N ** 2) * V_m) = {(N_m ** 2) / ((N ** 2) * V_m)}')\n",
    "    # print()\n",
    "    return -(2 * N_m * (N_m - 1)) / (N * (N - 1) * V_m) + (N_m ** 2) / ((N ** 2) * V_m)\n",
    "\n",
    "\n",
    "def calc_volume(bounding_box: Tuple[np.ndarray, np.ndarray]):\n",
    "    m, M = bounding_box\n",
    "    return np.prod(M - m)\n",
    "\n",
    "\n",
    "def make_density_split_node(node, N, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    N:    the total number of training instances for the current class\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "    m, M = node.box\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    e_min = float(\"inf\")\n",
    "    j_min, t_min = 0, 0\n",
    "\n",
    "    volume = calc_volume((m, M))\n",
    "\n",
    "    for j in feature_indices:\n",
    "        # Hint: For each feature considered, first remove duplicate feature values using\n",
    "        # 'np.unique()'. Describe here why this is necessary.\n",
    "\n",
    "        # It's necessary because otherwise if two instances have the same feature value the mean between them is the feature value self so the threshold would not be in the mid between feature values anymore\n",
    "\n",
    "        # np.unique returns an already sorted array\n",
    "        data_unique = np.unique(node.data[:, j])\n",
    "        # Compute candidate thresholds\n",
    "        tj = 0.5 * (data_unique[1:] + data_unique[:-1])\n",
    "\n",
    "        # Illustration: for loop - hint: vectorized version is possible\n",
    "        for t in tj:\n",
    "            # Compute the error\n",
    "            N_l = np.sum(node.data[:, j] <= t)\n",
    "            N_r = n - N_l\n",
    "\n",
    "            l_volume = volume * t / (M[j] - m[j])\n",
    "            r_volume = volume - l_volume\n",
    "\n",
    "            if l_volume != 0 and r_volume != 0:\n",
    "                loo_err_l = calc_loo_error(N_l, N, l_volume)\n",
    "                loo_err_r = calc_loo_error(N_r, N, r_volume)\n",
    "\n",
    "                loo_error = loo_err_l + loo_err_r\n",
    "\n",
    "                # print(f'loo_error: {loo_error}')\n",
    "\n",
    "                # choose the best threshold that\n",
    "                if loo_error < e_min:\n",
    "                    # print(f'e_min: {e_min}, j_min: {j_min}, t_min: {t_min}')\n",
    "                    e_min = loo_error\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "\n",
    "    # initialize 'left' and 'right' with the data subsets and bounding boxes\n",
    "    # according to the optimal split found above\n",
    "    left.data = node.data[node.data[:, j_min] <= t_min, :]  # store data in left node -- for subsequent splits\n",
    "    left.box = m.copy(), M.copy()  # store bounding box in left node\n",
    "    left.box[1][j_min] = t_min\n",
    "    right.data = node.data[node.data[:, j_min] > t_min, :]\n",
    "    right.box = m.copy(), M.copy()\n",
    "    right.box[0][j_min] = t_min\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_density_leaf_node(node, N):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    N:    the total number of training instances for the current class\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    n = node.data.shape[0]\n",
    "    v = calc_volume(node.box)\n",
    "    node.response = n / (N * v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree(Tree):\n",
    "    def __init__(self):\n",
    "        super(DecisionTree, self).__init__()\n",
    "\n",
    "    def train(self, data, labels, n_min=20):\n",
    "        '''\n",
    "        data: the feature matrix for all digits\n",
    "        labels: the corresponding ground-truth responses\n",
    "        n_min: termination criterion (don't split if a node contains fewer instances)\n",
    "        '''\n",
    "        N, D = data.shape\n",
    "        D_try = int(np.sqrt(D))  # how many features to consider for each split decision\n",
    "\n",
    "        # initialize the root node\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            n = node.data.shape[0]  # number of instances in present node\n",
    "            if n >= n_min and not node_is_pure(node):\n",
    "                # Call 'make_decision_split_node()' with 'D_try' randomly selected \n",
    "                # feature indices. This turns 'node' into a split node\n",
    "                # and returns the two children, which must be placed on the 'stack'.\n",
    "                indices = np.random.choice(np.arange(0, D), D_try)\n",
    "                left, right = make_decision_split_node(node, indices)\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "            else:\n",
    "                # Call 'make_decision_leaf_node()' to turn 'node' into a leaf node.\n",
    "                make_decision_leaf_node(node)\n",
    "\n",
    "    def predict(self, x):\n",
    "        leaf = self.find_leaf(x)\n",
    "        # compute p(y | x)\n",
    "        return np.argmax(leaf.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_gini(N_l_k: np.ndarray, N_l: int) -> float:\n",
    "    return N_l * (1 - (1 / N_l) * np.sum(N_l_k ** 2))\n",
    "\n",
    "\n",
    "def calc_N_l_k(node_target: np.ndarray, tresholded_indices: np.ndarray) -> np.ndarray:\n",
    "    return np.array([np.sum(node_target[tresholded_indices] == cls) for cls in range(10)])\n",
    "\n",
    "\n",
    "def make_decision_split_node(node, feature_indices):\n",
    "    '''\n",
    "    node: the node to be split\n",
    "    feature_indices: a numpy array of length 'D_try', containing the feature \n",
    "                     indices to be considered in the present split\n",
    "    '''\n",
    "    n, D = node.data.shape\n",
    "\n",
    "    e_min = float('inf')\n",
    "    t_min, j_min = 0, 0\n",
    "\n",
    "    # find best feature j (among 'feature_indices') and best threshold t for the split\n",
    "    for j in feature_indices:\n",
    "        data_unique = np.unique(node.data[:, j])\n",
    "        tj = 0.5 * (data_unique[1:] + data_unique[:-1])\n",
    "\n",
    "        for t in tj:\n",
    "            indices_l = np.where(node.data[:, j] <= t)\n",
    "            indices_r = np.where(node.data[:, j] > t)\n",
    "\n",
    "            N_l_k_l = calc_N_l_k(node.labels, indices_l)\n",
    "            N_l_k_r = calc_N_l_k(node.labels, indices_r)\n",
    "\n",
    "            N_l_l = len(indices_l)\n",
    "            N_l_r = len(indices_r)\n",
    "\n",
    "            gini_l = calc_gini(N_l_k_l, N_l_l)\n",
    "            gini_r = calc_gini(N_l_k_r, N_l_r)\n",
    "\n",
    "            gini = gini_l + gini_r\n",
    "\n",
    "            if gini < e_min:\n",
    "                e_min = gini\n",
    "                t_min = t\n",
    "                j_min = j\n",
    "\n",
    "    # create children\n",
    "    left = Node()\n",
    "    right = Node()\n",
    "\n",
    "    # initialize 'left' and 'right' with the data subsets and labels\n",
    "    # according to the optimal split found above\n",
    "    indices_l = np.where(node.data[:, j_min] <= t_min)\n",
    "    indices_r = np.where(node.data[:, j_min] > t_min)\n",
    "\n",
    "    left.data = node.data[indices_l]  # data in left node\n",
    "    left.labels = node.labels[indices_l]  # corresponding labels\n",
    "    right.data = node.data[indices_r]\n",
    "    right.labels = node.labels[indices_r]\n",
    "\n",
    "    # turn the current 'node' into a split node\n",
    "    # (store children and split condition)\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "    node.feature = j_min\n",
    "    node.threshold = t_min\n",
    "\n",
    "    # return the children (to be placed on the stack)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_decision_leaf_node(node):\n",
    "    '''\n",
    "    node: the node to become a leaf\n",
    "    '''\n",
    "    # compute and store leaf response\n",
    "    node.N = len(node.labels)\n",
    "    node.response = np.bincount(node.labels, minlength=10) / node.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def node_is_pure(node):\n",
    "    '''\n",
    "    check if 'node' ontains only instances of the same digit\n",
    "    '''\n",
    "    return len(np.unique(node.labels)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 33 34 35 36 37 38 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 59 60 61 62 63]\n",
      "(1797, 61)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "def calc_bbox(data) -> (np.ndarray, np.ndarray):\n",
    "    return np.min(data, axis=0).copy(), np.max(data, axis=0).copy()\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]\n",
    "\n",
    "# Removing features where min value == max value == 0, because this feature does not contain any information (it's the same for all instances)\n",
    "smallest, biggest = calc_bbox(data)\n",
    "distances = biggest - smallest\n",
    "print(distances.shape)\n",
    "\n",
    "dims_with_information = np.where(distances > 0)[0]\n",
    "print(dims_with_information)\n",
    "\n",
    "data = data[:, dims_with_information]\n",
    "print(data.shape)\n",
    "\n",
    "# Normalizing to values between 0 and 2\n",
    "# data = data / 16\n",
    "# print(calc_bbox(data))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# train trees, plot training error confusion matrices, and comment on your results\n",
    "\n",
    "def calc_confusion_matrix(calculated_target, expected) -> pd.DataFrame:\n",
    "    matrix = np.zeros((10, 10))\n",
    "    matrix_dict = {}\n",
    "    for expected_num in range(10):\n",
    "        expected_indices = np.where(expected == expected_num)[0]\n",
    "        calculated_values = calculated_target[expected_indices]\n",
    "        calc_bins = np.bincount(calculated_values, minlength=10)\n",
    "        matrix[expected_num] = calc_bins / len(expected_indices)\n",
    "        matrix_dict[expected_num] = calc_bins / len(expected_indices)\n",
    "\n",
    "    data_frame = pd.DataFrame(matrix_dict)\n",
    "    data_frame.columns.name = 'actual/expected'\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def show_plot(errors: np.ndarray, n_mins: np.ndarray):\n",
    "    x = range(len(n_mins))\n",
    "    x_ticks = np.arange(len(n_mins))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x, errors)\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(n_mins)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_test_density_tree(n_min) -> float:\n",
    "    density_trees: List[DensityTree] = []\n",
    "    for number in range(10):\n",
    "        indices = np.where(target == number)\n",
    "        filtered_data = data[indices]\n",
    "        prior = len(filtered_data) / len(data)\n",
    "\n",
    "        density_tree = DensityTree()\n",
    "        density_tree.train(filtered_data, prior, n_min)\n",
    "        density_trees.append(density_tree)\n",
    "\n",
    "    calculated_target = np.zeros(len(target), dtype=int)\n",
    "\n",
    "    for i, instance in enumerate(data):\n",
    "        p_max = -1\n",
    "        num_max = -1\n",
    "        for number, tree in enumerate(density_trees):\n",
    "            p = tree.predict(instance)\n",
    "            if p > p_max:\n",
    "                p_max = p\n",
    "                num_max = number\n",
    "        calculated_target[i] = num_max\n",
    "\n",
    "    # print(target)\n",
    "    # print(calculated_target)\n",
    "    density_tree_err = calculated_target != target\n",
    "    # print(density_tree_err)\n",
    "\n",
    "    density_tree_err_rate = np.sum(density_tree_err) / len(target)\n",
    "    confusion_matrix = calc_confusion_matrix(calculated_target, target)\n",
    "    print(density_tree_err_rate)\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    return density_tree_err_rate\n",
    "\n",
    "\n",
    "def train_test_decision_tree(n_min) -> float:\n",
    "    decision_tree = DecisionTree()\n",
    "    decision_tree.train(data, target, n_min)\n",
    "\n",
    "    calculated_target = np.array([decision_tree.predict(instance) for instance in data], dtype=int)\n",
    "\n",
    "    decision_tree_err = calculated_target != target\n",
    "    # print(decision_tree_err)\n",
    "\n",
    "    decision_tree_err_rate = np.sum(decision_tree_err) / len(target)\n",
    "    confusion_matrix = calc_confusion_matrix(calculated_target, target)\n",
    "    print(decision_tree_err_rate)\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    return decision_tree_err_rate\n",
    "\n",
    "\n",
    "n_mins = [40, 30, 20, 10, 5]\n",
    "density_tree_error = []\n",
    "print('Density tree:')\n",
    "for hyper_n_min in n_mins:\n",
    "    print(f'n_min: {hyper_n_min}')\n",
    "    error = train_test_density_tree(hyper_n_min)\n",
    "    density_tree_error.append(error)\n",
    "show_plot(density_tree_error, n_mins)\n",
    "\n",
    "decision_tree_error = []\n",
    "print()\n",
    "print('Decision tree:')\n",
    "for hyper_n_min in n_mins:\n",
    "    print(f'n_min: {hyper_n_min}')\n",
    "    error = train_test_decision_tree(hyper_n_min)\n",
    "    decision_tree_error.append(error)\n",
    "show_plot(decision_tree_error, n_mins)\n"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density tree:\n",
      "n_min: 40\n",
      "0.20311630495269895\n",
      "actual/expected    0         1         2         3         4         5    6  \\\n",
      "0                1.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "1                0.0  0.714286  0.101695  0.027322  0.000000  0.010989  0.0   \n",
      "2                0.0  0.000000  0.451977  0.021858  0.000000  0.000000  0.0   \n",
      "3                0.0  0.000000  0.022599  0.704918  0.000000  0.021978  0.0   \n",
      "4                0.0  0.082418  0.000000  0.000000  0.861878  0.000000  0.0   \n",
      "5                0.0  0.000000  0.022599  0.038251  0.011050  0.862637  0.0   \n",
      "6                0.0  0.000000  0.000000  0.000000  0.000000  0.000000  1.0   \n",
      "7                0.0  0.005495  0.000000  0.032787  0.116022  0.032967  0.0   \n",
      "8                0.0  0.197802  0.401130  0.174863  0.005525  0.043956  0.0   \n",
      "9                0.0  0.000000  0.000000  0.000000  0.005525  0.027473  0.0   \n",
      "\n",
      "actual/expected         7         8         9  \n",
      "0                0.000000  0.000000  0.000000  \n",
      "1                0.016760  0.114943  0.027778  \n",
      "2                0.000000  0.000000  0.000000  \n",
      "3                0.000000  0.000000  0.188889  \n",
      "4                0.011173  0.000000  0.038889  \n",
      "5                0.000000  0.000000  0.011111  \n",
      "6                0.000000  0.000000  0.000000  \n",
      "7                0.966480  0.017241  0.066667  \n",
      "8                0.005587  0.867816  0.127778  \n",
      "9                0.000000  0.000000  0.538889  \n",
      "n_min: 30\n",
      "0.2587646076794658\n",
      "actual/expected    0         1         2         3         4         5    6  \\\n",
      "0                1.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "1                0.0  0.505495  0.056497  0.010929  0.005525  0.010989  0.0   \n",
      "2                0.0  0.071429  0.384181  0.021858  0.000000  0.000000  0.0   \n",
      "3                0.0  0.005495  0.248588  0.502732  0.000000  0.065934  0.0   \n",
      "4                0.0  0.010989  0.000000  0.000000  0.861878  0.000000  0.0   \n",
      "5                0.0  0.000000  0.028249  0.054645  0.011050  0.719780  0.0   \n",
      "6                0.0  0.000000  0.000000  0.000000  0.000000  0.000000  1.0   \n",
      "7                0.0  0.000000  0.000000  0.021858  0.110497  0.054945  0.0   \n",
      "8                0.0  0.373626  0.282486  0.284153  0.005525  0.115385  0.0   \n",
      "9                0.0  0.032967  0.000000  0.103825  0.005525  0.032967  0.0   \n",
      "\n",
      "actual/expected         7         8         9  \n",
      "0                0.000000  0.000000  0.000000  \n",
      "1                0.000000  0.022989  0.027778  \n",
      "2                0.000000  0.005747  0.000000  \n",
      "3                0.000000  0.074713  0.222222  \n",
      "4                0.005587  0.000000  0.027778  \n",
      "5                0.005587  0.005747  0.022222  \n",
      "6                0.000000  0.000000  0.000000  \n",
      "7                0.983240  0.000000  0.061111  \n",
      "8                0.005587  0.890805  0.066667  \n",
      "9                0.000000  0.000000  0.572222  \n",
      "n_min: 20\n",
      "0.21480244852532\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.988764  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1                0.000000  0.708791  0.033898  0.021858  0.000000  0.016484   \n",
      "2                0.000000  0.065934  0.519774  0.005464  0.000000  0.000000   \n",
      "3                0.000000  0.000000  0.107345  0.568306  0.000000  0.027473   \n",
      "4                0.005618  0.032967  0.000000  0.005464  0.906077  0.000000   \n",
      "5                0.000000  0.005495  0.022599  0.010929  0.000000  0.675824   \n",
      "6                0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7                0.000000  0.016484  0.000000  0.038251  0.082873  0.104396   \n",
      "8                0.005618  0.170330  0.316384  0.300546  0.000000  0.082418   \n",
      "9                0.000000  0.000000  0.000000  0.049180  0.011050  0.093407   \n",
      "\n",
      "actual/expected    6         7         8         9  \n",
      "0                0.0  0.000000  0.000000  0.000000  \n",
      "1                0.0  0.005587  0.040230  0.050000  \n",
      "2                0.0  0.000000  0.000000  0.005556  \n",
      "3                0.0  0.000000  0.005747  0.038889  \n",
      "4                0.0  0.044693  0.022989  0.011111  \n",
      "5                0.0  0.000000  0.011494  0.011111  \n",
      "6                1.0  0.000000  0.000000  0.000000  \n",
      "7                0.0  0.949721  0.074713  0.077778  \n",
      "8                0.0  0.000000  0.844828  0.111111  \n",
      "9                0.0  0.000000  0.000000  0.694444  \n",
      "n_min: 10\n",
      "0.20923761825264328\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.988764  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1                0.000000  0.725275  0.011299  0.005464  0.000000  0.021978   \n",
      "2                0.000000  0.027473  0.480226  0.000000  0.000000  0.000000   \n",
      "3                0.000000  0.000000  0.305085  0.606557  0.000000  0.038462   \n",
      "4                0.011236  0.065934  0.000000  0.000000  0.828729  0.016484   \n",
      "5                0.000000  0.000000  0.016949  0.016393  0.000000  0.785714   \n",
      "6                0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7                0.000000  0.032967  0.000000  0.016393  0.149171  0.016484   \n",
      "8                0.000000  0.131868  0.186441  0.333333  0.000000  0.104396   \n",
      "9                0.000000  0.016484  0.000000  0.021858  0.022099  0.016484   \n",
      "\n",
      "actual/expected         6         7         8         9  \n",
      "0                0.000000  0.000000  0.000000  0.000000  \n",
      "1                0.005525  0.000000  0.022989  0.027778  \n",
      "2                0.000000  0.000000  0.000000  0.000000  \n",
      "3                0.000000  0.005587  0.017241  0.138889  \n",
      "4                0.000000  0.016760  0.005747  0.011111  \n",
      "5                0.000000  0.005587  0.000000  0.000000  \n",
      "6                0.988950  0.000000  0.000000  0.000000  \n",
      "7                0.000000  0.944134  0.022989  0.038889  \n",
      "8                0.005525  0.011173  0.919540  0.138889  \n",
      "9                0.000000  0.016760  0.011494  0.644444  \n",
      "n_min: 5\n",
      "0.18308291597106288\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.994382  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1                0.000000  0.802198  0.028249  0.000000  0.000000  0.005495   \n",
      "2                0.000000  0.016484  0.542373  0.000000  0.000000  0.000000   \n",
      "3                0.000000  0.005495  0.067797  0.606557  0.000000  0.054945   \n",
      "4                0.005618  0.016484  0.000000  0.000000  0.928177  0.000000   \n",
      "5                0.000000  0.005495  0.000000  0.021858  0.005525  0.763736   \n",
      "6                0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7                0.000000  0.000000  0.000000  0.010929  0.060773  0.010989   \n",
      "8                0.000000  0.153846  0.338983  0.229508  0.000000  0.054945   \n",
      "9                0.000000  0.000000  0.022599  0.131148  0.005525  0.109890   \n",
      "\n",
      "actual/expected         6         7         8         9  \n",
      "0                0.000000  0.000000  0.000000  0.000000  \n",
      "1                0.000000  0.000000  0.068966  0.016667  \n",
      "2                0.000000  0.000000  0.000000  0.000000  \n",
      "3                0.000000  0.011173  0.005747  0.083333  \n",
      "4                0.000000  0.016760  0.005747  0.011111  \n",
      "5                0.000000  0.000000  0.011494  0.000000  \n",
      "6                0.994475  0.000000  0.000000  0.000000  \n",
      "7                0.000000  0.949721  0.011494  0.050000  \n",
      "8                0.005525  0.011173  0.850575  0.100000  \n",
      "9                0.000000  0.011173  0.045977  0.738889  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision tree:\n",
      "n_min: 40\n",
      "0.27378964941569284\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.910112  0.000000  0.005650  0.005464  0.049724  0.010989   \n",
      "1                0.000000  0.862637  0.073446  0.010929  0.077348  0.032967   \n",
      "2                0.000000  0.038462  0.683616  0.098361  0.000000  0.071429   \n",
      "3                0.022472  0.021978  0.101695  0.650273  0.000000  0.148352   \n",
      "4                0.016854  0.010989  0.005650  0.010929  0.812155  0.021978   \n",
      "5                0.005618  0.010989  0.016949  0.065574  0.016575  0.598901   \n",
      "6                0.033708  0.010989  0.084746  0.071038  0.011050  0.027473   \n",
      "7                0.005618  0.000000  0.000000  0.010929  0.027624  0.010989   \n",
      "8                0.000000  0.032967  0.022599  0.038251  0.000000  0.027473   \n",
      "9                0.005618  0.010989  0.005650  0.038251  0.005525  0.049451   \n",
      "\n",
      "actual/expected         6         7         8         9  \n",
      "0                0.016575  0.011173  0.011494  0.033333  \n",
      "1                0.038674  0.011173  0.028736  0.077778  \n",
      "2                0.038674  0.016760  0.080460  0.044444  \n",
      "3                0.027624  0.016760  0.155172  0.044444  \n",
      "4                0.027624  0.089385  0.017241  0.011111  \n",
      "5                0.000000  0.061453  0.080460  0.055556  \n",
      "6                0.828729  0.011173  0.040230  0.077778  \n",
      "7                0.000000  0.759777  0.005747  0.033333  \n",
      "8                0.022099  0.016760  0.551724  0.022222  \n",
      "9                0.000000  0.005587  0.028736  0.600000  \n",
      "n_min: 30\n",
      "0.34668892598775736\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.898876  0.005495  0.011299  0.021858  0.022099  0.005495   \n",
      "1                0.000000  0.747253  0.084746  0.000000  0.066298  0.016484   \n",
      "2                0.000000  0.027473  0.621469  0.136612  0.011050  0.093407   \n",
      "3                0.016854  0.021978  0.112994  0.497268  0.005525  0.109890   \n",
      "4                0.028090  0.027473  0.011299  0.000000  0.745856  0.043956   \n",
      "5                0.005618  0.021978  0.112994  0.087432  0.016575  0.609890   \n",
      "6                0.011236  0.060440  0.022599  0.065574  0.022099  0.016484   \n",
      "7                0.022472  0.054945  0.000000  0.060109  0.099448  0.065934   \n",
      "8                0.005618  0.032967  0.022599  0.049180  0.000000  0.005495   \n",
      "9                0.011236  0.000000  0.000000  0.081967  0.011050  0.032967   \n",
      "\n",
      "actual/expected         6         7         8         9  \n",
      "0                0.022099  0.022346  0.034483  0.050000  \n",
      "1                0.016575  0.011173  0.011494  0.044444  \n",
      "2                0.022099  0.005587  0.103448  0.061111  \n",
      "3                0.055249  0.016760  0.114943  0.072222  \n",
      "4                0.016575  0.117318  0.040230  0.038889  \n",
      "5                0.011050  0.044693  0.051724  0.055556  \n",
      "6                0.779006  0.033520  0.103448  0.033333  \n",
      "7                0.016575  0.715084  0.045977  0.122222  \n",
      "8                0.044199  0.005587  0.465517  0.072222  \n",
      "9                0.016575  0.027933  0.028736  0.450000  \n",
      "n_min: 20\n",
      "0.29493600445186424\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.814607  0.005495  0.011299  0.005464  0.060773  0.010989   \n",
      "1                0.022472  0.813187  0.073446  0.016393  0.022099  0.010989   \n",
      "2                0.022472  0.005495  0.655367  0.109290  0.005525  0.043956   \n",
      "3                0.000000  0.038462  0.039548  0.617486  0.000000  0.038462   \n",
      "4                0.011236  0.021978  0.000000  0.005464  0.845304  0.032967   \n",
      "5                0.016854  0.049451  0.141243  0.087432  0.016575  0.725275   \n",
      "6                0.005618  0.000000  0.056497  0.049180  0.000000  0.027473   \n",
      "7                0.016854  0.027473  0.000000  0.021858  0.016575  0.049451   \n",
      "8                0.044944  0.038462  0.016949  0.054645  0.011050  0.016484   \n",
      "9                0.044944  0.000000  0.005650  0.032787  0.022099  0.043956   \n",
      "\n",
      "actual/expected         6         7         8         9  \n",
      "0                0.038674  0.016760  0.028736  0.050000  \n",
      "1                0.055249  0.027933  0.057471  0.033333  \n",
      "2                0.033149  0.011173  0.063218  0.044444  \n",
      "3                0.011050  0.011173  0.040230  0.061111  \n",
      "4                0.022099  0.139665  0.017241  0.027778  \n",
      "5                0.000000  0.055866  0.086207  0.100000  \n",
      "6                0.828729  0.000000  0.080460  0.088889  \n",
      "7                0.005525  0.726257  0.028736  0.072222  \n",
      "8                0.000000  0.005587  0.574713  0.077778  \n",
      "9                0.005525  0.005587  0.022989  0.444444  \n",
      "n_min: 10\n",
      "0.20311630495269895\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.943820  0.016484  0.000000  0.000000  0.011050  0.000000   \n",
      "1                0.000000  0.890110  0.028249  0.021858  0.005525  0.043956   \n",
      "2                0.011236  0.010989  0.779661  0.071038  0.022099  0.038462   \n",
      "3                0.005618  0.000000  0.079096  0.743169  0.011050  0.065934   \n",
      "4                0.016854  0.000000  0.005650  0.005464  0.878453  0.021978   \n",
      "5                0.000000  0.021978  0.022599  0.032787  0.022099  0.747253   \n",
      "6                0.005618  0.005495  0.033898  0.043716  0.016575  0.016484   \n",
      "7                0.000000  0.005495  0.000000  0.010929  0.016575  0.032967   \n",
      "8                0.005618  0.027473  0.039548  0.043716  0.016575  0.021978   \n",
      "9                0.011236  0.021978  0.011299  0.027322  0.000000  0.010989   \n",
      "\n",
      "actual/expected         6         7         8         9  \n",
      "0                0.000000  0.022346  0.000000  0.016667  \n",
      "1                0.016575  0.022346  0.057471  0.027778  \n",
      "2                0.027624  0.005587  0.103448  0.022222  \n",
      "3                0.038674  0.016760  0.068966  0.016667  \n",
      "4                0.000000  0.094972  0.017241  0.061111  \n",
      "5                0.005525  0.027933  0.028736  0.033333  \n",
      "6                0.883978  0.005587  0.045977  0.066667  \n",
      "7                0.000000  0.770950  0.011494  0.050000  \n",
      "8                0.022099  0.005587  0.649425  0.027778  \n",
      "9                0.005525  0.027933  0.017241  0.677778  \n",
      "n_min: 5\n",
      "0.12576516416249303\n",
      "actual/expected         0         1         2         3         4         5  \\\n",
      "0                0.971910  0.000000  0.000000  0.000000  0.027624  0.005495   \n",
      "1                0.000000  0.939560  0.039548  0.016393  0.016575  0.043956   \n",
      "2                0.000000  0.010989  0.909605  0.043716  0.016575  0.021978   \n",
      "3                0.000000  0.005495  0.016949  0.885246  0.005525  0.049451   \n",
      "4                0.011236  0.005495  0.000000  0.000000  0.933702  0.010989   \n",
      "5                0.011236  0.010989  0.005650  0.010929  0.000000  0.851648   \n",
      "6                0.000000  0.000000  0.000000  0.005464  0.000000  0.000000   \n",
      "7                0.000000  0.000000  0.000000  0.005464  0.000000  0.000000   \n",
      "8                0.005618  0.021978  0.022599  0.016393  0.000000  0.010989   \n",
      "9                0.000000  0.005495  0.005650  0.016393  0.000000  0.005495   \n",
      "\n",
      "actual/expected         6         7         8         9  \n",
      "0                0.005525  0.016760  0.000000  0.016667  \n",
      "1                0.016575  0.011173  0.057471  0.016667  \n",
      "2                0.005525  0.000000  0.040230  0.016667  \n",
      "3                0.005525  0.022346  0.080460  0.100000  \n",
      "4                0.016575  0.016760  0.000000  0.005556  \n",
      "5                0.000000  0.055866  0.028736  0.044444  \n",
      "6                0.944751  0.005587  0.011494  0.011111  \n",
      "7                0.000000  0.849162  0.005747  0.022222  \n",
      "8                0.005525  0.016760  0.758621  0.072222  \n",
      "9                0.000000  0.005587  0.017241  0.694444  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The adjustment of the hyperparameter has the same form for both the density tree and the decision tree. Thus the deviation in the errors is for the decision tree broader.\n",
    "Both perform best with smaller n_min and the decision tree with n_min = 5 performs best"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DensityForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DensityTree() for i in range(n_trees)]\n",
    "\n",
    "    def train(self, data, prior, n_min=20):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            ...  # your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return ...  # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DecisionForest():\n",
    "    def __init__(self, n_trees):\n",
    "        # create ensemble\n",
    "        self.trees = [DecisionTree() for i in range(n_trees)]\n",
    "\n",
    "    def train(self, data, labels, n_min=0):\n",
    "        for tree in self.trees:\n",
    "            # train each tree, using a bootstrap sample of the data\n",
    "            ...  # your code here\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the ensemble prediction\n",
    "        return ...  # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Density and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train forests (with 20 trees per forest), plot training error confusion matrices, and comment on your results\n",
    "...  # your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}